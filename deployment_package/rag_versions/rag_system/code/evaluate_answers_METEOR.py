#!/usr/bin/env python3
"""
METEOR Evaluator for Real Answers
Computes METEOR score for answers generated by generate_answers.py

Usage:
  python evaluate_answers_METEOR.py <results_csv>

This script expects a CSV with at least the following columns (robust to naming):
- Question (or 'question')
- Generated Answer (often: 'Answer', 'Model_Answer', 'Generated_Answer', 'Real_LLaMA_Answer')
- Ground Truth (often: 'Ground Truth Answer', 'Ground_Truth', 'Ground_Truth_Answer', 'Answer - Reviewed')

Outputs a CSV in the evaluation/METEOR folder with METEOR metrics per row, and prints macro averages.
"""

import os
import sys
import pandas as pd
from datetime import datetime
import nltk
from nltk.translate.meteor_score import meteor_score
from nltk.tokenize import word_tokenize

# Ensure NLTK data is available
try:
    nltk.data.find('corpora/wordnet.zip')
except LookupError:
    nltk.download('wordnet', quiet=True)
try:
    nltk.data.find('corpora/omw-1.4.zip')
except LookupError:
    nltk.download('omw-1.4', quiet=True)
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt', quiet=True)
try:
    nltk.data.find('tokenizers/punkt_tab')
except LookupError:
    nltk.download('punkt_tab', quiet=True)

# -------------------- Tokenization --------------------

def tokenize(text: str):
    t = (text or '').strip().lower()
    try:
        tokens = word_tokenize(t)
    except Exception:
        tokens = t.split()
    # filter out pure punctuation tokens (keep tokens containing alphanumerics)
    return [w for w in tokens if any(ch.isalnum() for ch in w)]

# -------------------- Column resolution --------------------

def resolve_column(df, candidates, default=None):
    cols = {c.lower(): c for c in df.columns}
    for cand in candidates:
        key = cand.lower()
        if key in cols:
            return cols[key]
    return default

# -------------------- Main evaluation --------------------

def evaluate_meteor(results_csv: str):
    if not os.path.exists(results_csv):
        print(f"‚ùå CSV not found: {results_csv}")
        return None
    
    print(f"üìñ Reading CSV: {results_csv}")
    df = pd.read_csv(results_csv)
    
    # try to find columns
    q_col = resolve_column(df, ['Question', 'question'])
    a_col = resolve_column(df, ['Real_LLaMA_Answer', 'Answer', 'Model_Answer', 'Generated_Answer', 'Answer_Text'])
    gt_col = resolve_column(df, [
        'Ground Truth Answer', 'Ground_Truth', 'Ground_Truth_Answer', 'Answer - Reviewed', 'GroundTruth', 'Reviewed_Answer', 'Answer'
    ])
    qid_col = resolve_column(df, ['Question_ID', 'id', 'ID', 'QID', 'Question ID'])

    if not q_col or not a_col or not gt_col:
        print(f"‚ö†Ô∏è Columns not resolved. Found cols={list(df.columns)}")
        print(f"   Need at least Question, Generated Answer, and Ground Truth.")
        return None

    print(f"‚úÖ Columns resolved: Question='{q_col}', Answer='{a_col}', GroundTruth='{gt_col}'")

    rows = []
    meteor_sum = 0.0
    count = 0

    print(f"üöÄ Computing METEOR for {len(df)} samples...")

    for _, r in df.iterrows():
        question = str(r.get(q_col, '') or '')
        gen = str(r.get(a_col, '') or '')
        gt = str(r.get(gt_col, '') or '')
        qid = str(r.get(qid_col, '') or '') if qid_col else ''

        if not gen.strip() or not gt.strip():
            continue
        
        cand_toks = tokenize(gen)
        ref_toks = tokenize(gt)
        
        # meteor_score expects reference as a list of list of tokens (multiple references supported), 
        # and candidate as a list of tokens.
        # Note: In newer NLTK versions, it might expect list of strings. 
        # Let's check usage. NLTK's meteor_score takes (references, hypothesis).
        # references is list of strings or list of list of strings (tokens).
        # hypothesis is string or list of strings (tokens).
        # To be safe and consistent with tokenization, we pass lists of tokens.
        
        try:
            score = meteor_score([ref_toks], cand_toks)
        except Exception as e:
            print(f"‚ö†Ô∏è METEOR calculation error: {e}")
            score = 0.0
        
        rows.append({
            'Question_ID': qid,
            'Question': question,
            'Ground_Truth_Answer': gt,
            'Real_LLaMA_Answer': gen,
            'METEOR': score
        })
        meteor_sum += score
        count += 1

    if count == 0:
        print("‚ö†Ô∏è No evaluable rows (missing answers or ground truth).")
        return None

    avg_meteor = meteor_sum / count

    ts = datetime.now().strftime('%Y%m%d_%H%M%S')
    out_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'evaluation', 'METEOR'))
    os.makedirs(out_dir, exist_ok=True)
    out_csv = os.path.join(out_dir, f"llama_evaluation_meteor_{ts}.csv")
    pd.DataFrame(rows).to_csv(out_csv, index=False)

    print("\n=== METEOR Evaluation Summary ===")
    print(f"File: {results_csv}")
    print(f"Evaluated rows: {count}")
    print(f"METEOR (avg): {avg_meteor:.4f}")
    print(f"üìÅ Saved per-question METEOR to: {out_csv}")
    return out_csv


if __name__ == '__main__':
    if len(sys.argv) < 2:
        print("Usage: python evaluate_answers_METEOR.py <results_csv>")
        sys.exit(1)
    evaluate_meteor(sys.argv[1])
