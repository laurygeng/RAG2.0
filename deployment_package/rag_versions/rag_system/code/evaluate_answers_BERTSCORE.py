#!/usr/bin/env python3
"""
BERTSCORE Evaluator for Real Answers
Computes BERTScore (Precision, Recall, F1) for answers generated by generate_answers.py

Usage:
  python evaluate_answers_BERTSCORE.py <results_csv>

This script expects a CSV with at least the following columns (robust to naming):
- Question (or 'question')
- Generated Answer (often: 'Answer', 'Model_Answer', 'Generated_Answer', 'Real_LLaMA_Answer')
- Ground Truth (often: 'Ground Truth Answer', 'Ground_Truth', 'Ground_Truth_Answer', 'Answer - Reviewed')

Outputs a CSV in the evaluation/BERTSCORE folder with BERTScore metrics per row, and prints macro averages.
"""

import os
import sys
import pandas as pd
from datetime import datetime
from bert_score import score
import torch

# -------------------- Column resolution --------------------

def resolve_column(df, candidates, default=None):
    cols = {c.lower(): c for c in df.columns}
    for cand in candidates:
        key = cand.lower()
        if key in cols:
            return cols[key]
    return default

# -------------------- Main evaluation --------------------

def evaluate_bertscore(results_csv: str):
    if not os.path.exists(results_csv):
        print(f"‚ùå CSV not found: {results_csv}")
        return None
    
    print(f"üìñ Reading CSV: {results_csv}")
    df = pd.read_csv(results_csv)
    
    # try to find columns
    q_col = resolve_column(df, ['Question', 'question'])
    a_col = resolve_column(df, ['Real_LLaMA_Answer', 'Answer', 'Model_Answer', 'Generated_Answer', 'Answer_Text'])
    gt_col = resolve_column(df, [
        'Ground Truth Answer', 'Ground_Truth', 'Ground_Truth_Answer', 'Answer - Reviewed', 'GroundTruth', 'Reviewed_Answer', 'Answer'
    ])
    qid_col = resolve_column(df, ['Question_ID', 'id', 'ID', 'QID', 'Question ID'])

    if not q_col or not a_col or not gt_col:
        print(f"‚ö†Ô∏è Columns not resolved. Found cols={list(df.columns)}")
        print(f"   Need at least Question, Generated Answer, and Ground Truth.")
        return None

    print(f"‚úÖ Columns resolved: Question='{q_col}', Answer='{a_col}', GroundTruth='{gt_col}'")

    # Ensure columns are strings to avoid .str accessor errors
    df[a_col] = df[a_col].astype(str)
    df[gt_col] = df[gt_col].astype(str)

    # Filter valid rows
    valid_df = df[df[a_col].notna() & df[gt_col].notna() & (df[a_col].str.strip() != '') & (df[gt_col].str.strip() != '')].copy()
    
    if len(valid_df) == 0:
        print("‚ö†Ô∏è No evaluable rows (missing answers or ground truth).")
        return None

    cands = valid_df[a_col].tolist()
    refs = valid_df[gt_col].tolist()

    print(f"üöÄ Computing BERTScore for {len(cands)} samples...")
    # Compute BERTScore
    # lang="en" uses roberta-large by default for English
    # rescale_with_baseline=True is often recommended but requires baseline files. 
    # For simplicity and standard usage, we'll stick to default (rescale_with_baseline=False).
    # Added batch_size=32 to prevent OOM on large datasets
    try:
        P, R, F1 = score(cands, refs, lang="en", verbose=True, batch_size=32)
    except Exception as e:
        print(f"‚ùå BERTScore computation failed: {e}")
        return None

    # Add scores to dataframe
    valid_df['BERTSCORE_Precision'] = P.tolist()
    valid_df['BERTSCORE_Recall'] = R.tolist()
    valid_df['BERTSCORE_F1'] = F1.tolist()

    # Prepare output dataframe
    out_cols = []
    if qid_col:
        out_cols.append(qid_col)
    out_cols.extend([q_col, gt_col, a_col, 'BERTSCORE_Precision', 'BERTSCORE_Recall', 'BERTSCORE_F1'])
    
    out_df = valid_df[out_cols]

    # Calculate averages
    avg_p = P.mean().item()
    avg_r = R.mean().item()
    avg_f1 = F1.mean().item()

    ts = datetime.now().strftime('%Y%m%d_%H%M%S')
    out_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'evaluation', 'BERTSCORE'))
    os.makedirs(out_dir, exist_ok=True)
    out_csv = os.path.join(out_dir, f"llama_evaluation_bertscore_{ts}.csv")
    out_df.to_csv(out_csv, index=False)

    print("\n=== BERTScore Evaluation Summary ===")
    print(f"File: {results_csv}")
    print(f"Evaluated rows: {len(valid_df)}")
    print(f"BERTScore Precision (avg): {avg_p:.4f}")
    print(f"BERTScore Recall (avg):    {avg_r:.4f}")
    print(f"BERTScore F1 (avg):        {avg_f1:.4f}")
    print(f"üìÅ Saved per-question BERTScore to: {out_csv}")
    return out_csv


if __name__ == '__main__':
    if len(sys.argv) < 2:
        print("Usage: python evaluate_answers_BERTSCORE.py <results_csv>")
        sys.exit(1)
    evaluate_bertscore(sys.argv[1])
