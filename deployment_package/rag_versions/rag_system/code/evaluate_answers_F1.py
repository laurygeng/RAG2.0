#!/usr/bin/env python3
"""
F1 Evaluator for Real Answers (Token-level)
Computes Token-level Precision, Recall, and F1 for answers generated by generate_answers.py

Usage:
  python evaluate_answers_F1.py <results_csv>

This script expects a CSV with at least the following columns (robust to naming):
- Question (or 'question')
- Generated Answer (often: 'Answer', 'Model_Answer', 'Generated_Answer', 'Real_LLaMA_Answer')
- Ground Truth (often: 'Ground Truth Answer', 'Ground_Truth', 'Ground_Truth_Answer', 'Answer - Reviewed')

Outputs a CSV in the evaluation/F1 folder with F1 metrics per row, and prints macro averages.
"""

import os
import sys
import pandas as pd
from datetime import datetime
import collections
import string
import nltk
from nltk.tokenize import word_tokenize

# Ensure NLTK data is available
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt', quiet=True)
try:
    nltk.data.find('tokenizers/punkt_tab')
except LookupError:
    nltk.download('punkt_tab', quiet=True)

# -------------------- Tokenization & Normalization --------------------

def normalize_answer(s):
    """Lower text and remove extra whitespace. Punctuation is preserved."""
    def white_space_fix(text):
        return ' '.join(text.split())

    def lower(text):
        return text.lower()

    # We preserve punctuation and articles now to allow token-level alignment
    return white_space_fix(lower(s))

def get_tokens(s):
    if not s: return []
    return word_tokenize(normalize_answer(s))

# -------------------- F1 Computation --------------------

def compute_f1(a_gold, a_pred):
    gold_toks = get_tokens(a_gold)
    pred_toks = get_tokens(a_pred)
    
    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)
    num_same = sum(common.values())
    
    if len(gold_toks) == 0 or len(pred_toks) == 0:
        # If either is empty, F1 is 1 if both are empty, else 0
        return int(gold_toks == pred_toks), int(gold_toks == pred_toks), int(gold_toks == pred_toks)
    
    if num_same == 0:
        return 0.0, 0.0, 0.0
    
    precision = 1.0 * num_same / len(pred_toks)
    recall = 1.0 * num_same / len(gold_toks)
    f1 = (2 * precision * recall) / (precision + recall)
    
    return precision, recall, f1

# -------------------- Column resolution --------------------

def resolve_column(df, candidates, default=None):
    cols = {c.lower(): c for c in df.columns}
    for cand in candidates:
        key = cand.lower()
        if key in cols:
            return cols[key]
    return default

# -------------------- Main evaluation --------------------

def evaluate_f1(results_csv: str):
    if not os.path.exists(results_csv):
        print(f"‚ùå CSV not found: {results_csv}")
        return None
    
    print(f"üìñ Reading CSV: {results_csv}")
    df = pd.read_csv(results_csv)
    
    # try to find columns
    q_col = resolve_column(df, ['Question', 'question'])
    a_col = resolve_column(df, ['Real_LLaMA_Answer', 'Answer', 'Model_Answer', 'Generated_Answer', 'Answer_Text'])
    gt_col = resolve_column(df, [
        'Ground Truth Answer', 'Ground_Truth', 'Ground_Truth_Answer', 'Answer - Reviewed', 'GroundTruth', 'Reviewed_Answer', 'Answer'
    ])
    qid_col = resolve_column(df, ['Question_ID', 'id', 'ID', 'QID', 'Question ID'])

    if not q_col or not a_col or not gt_col:
        print(f"‚ö†Ô∏è Columns not resolved. Found cols={list(df.columns)}")
        print(f"   Need at least Question, Generated Answer, and Ground Truth.")
        return None

    print(f"‚úÖ Columns resolved: Question='{q_col}', Answer='{a_col}', GroundTruth='{gt_col}'")

    rows = []
    p_sum = r_sum = f1_sum = 0.0
    count = 0

    print(f"üöÄ Computing Token F1 for {len(df)} samples...")

    for _, r in df.iterrows():
        question = str(r.get(q_col, '') or '')
        gen = str(r.get(a_col, '') or '')
        gt = str(r.get(gt_col, '') or '')
        qid = str(r.get(qid_col, '') or '') if qid_col else ''

        if not gen.strip() or not gt.strip():
            continue
        
        p, r_val, f1 = compute_f1(gt, gen)
        
        rows.append({
            'Question_ID': qid,
            'Question': question,
            'Ground_Truth_Answer': gt,
            'Real_LLaMA_Answer': gen,
            'Token_Precision': p,
            'Token_Recall': r_val,
            'Token_F1': f1
        })
        p_sum += p
        r_sum += r_val
        f1_sum += f1
        count += 1

    if count == 0:
        print("‚ö†Ô∏è No evaluable rows (missing answers or ground truth).")
        return None

    avg_p = p_sum / count
    avg_r = r_sum / count
    avg_f1 = f1_sum / count

    ts = datetime.now().strftime('%Y%m%d_%H%M%S')
    out_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'evaluation', 'F1'))
    os.makedirs(out_dir, exist_ok=True)
    out_csv = os.path.join(out_dir, f"llama_evaluation_f1_{ts}.csv")
    pd.DataFrame(rows).to_csv(out_csv, index=False)

    print("\n=== Token F1 Evaluation Summary ===")
    print(f"File: {results_csv}")
    print(f"Evaluated rows: {count}")
    print(f"Token Precision (avg): {avg_p:.4f}")
    print(f"Token Recall (avg):    {avg_r:.4f}")
    print(f"Token F1 (avg):        {avg_f1:.4f}")
    print(f"üìÅ Saved per-question F1 to: {out_csv}")
    return out_csv


if __name__ == '__main__':
    if len(sys.argv) < 2:
        print("Usage: python evaluate_answers_F1.py <results_csv>")
        sys.exit(1)
    evaluate_f1(sys.argv[1])
