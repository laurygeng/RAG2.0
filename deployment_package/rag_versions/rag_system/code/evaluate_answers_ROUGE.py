#!/usr/bin/env python3
"""
ROUGE Evaluator for Real Answers
Computes ROUGE-1, ROUGE-2, and ROUGE-L metrics for answers generated by generate_answers.py

Usage:
  python evaluate_answers_ROUGE.py <results_csv>

This script expects a CSV with at least the following columns (robust to naming):
- Question (or 'question')
- Generated Answer (often: 'Answer', 'Model_Answer', 'Generated_Answer', 'Real_LLaMA_Answer')
- Ground Truth (often: 'Ground Truth Answer', 'Ground_Truth', 'Ground_Truth_Answer', 'Answer - Reviewed')

Outputs a CSV in the evaluation/ROUGE folder with metrics per row, and prints macro averages.
"""

import os
import sys
import pandas as pd
from datetime import datetime
import numpy as np

# Try to import rouge_score
try:
    from rouge_score import rouge_scorer
except ImportError:
    print("‚ùå rouge_score library not found.")
    print("Please install it via: pip install rouge-score")
    sys.exit(1)

# -------------------- Column resolution --------------------

def resolve_column(df, candidates, default=None):
    cols = {c.lower(): c for c in df.columns}
    for cand in candidates:
        key = cand.lower()
        if key in cols:
            return cols[key]
    return default

# -------------------- Main evaluation --------------------

def evaluate_rouge(results_csv: str):
    if not os.path.exists(results_csv):
        print(f"‚ùå CSV not found: {results_csv}")
        return None
    
    print(f"üìñ Reading CSV: {results_csv}")
    df = pd.read_csv(results_csv)
    
    # try to find columns
    q_col = resolve_column(df, ['Question', 'question'])
    a_col = resolve_column(df, ['Real_LLaMA_Answer', 'Answer', 'Model_Answer', 'Generated_Answer', 'Answer_Text'])
    gt_col = resolve_column(df, [
        'Ground Truth Answer', 'Ground_Truth', 'Ground_Truth_Answer', 'Answer - Reviewed', 'GroundTruth', 'Reviewed_Answer', 'Answer'
    ])
    qid_col = resolve_column(df, ['Question_ID', 'id', 'ID', 'QID', 'Question ID'])

    if not q_col or not a_col or not gt_col:
        print(f"‚ö†Ô∏è Columns not resolved. Found cols={list(df.columns)}")
        print(f"   Need at least Question, Generated Answer, and Ground Truth.")
        return None

    print(f"‚úÖ Columns resolved: Question='{q_col}', Answer='{a_col}', GroundTruth='{gt_col}'")

    # Initialize scorer
    # use_stemmer=True is standard for ROUGE to map word variants to the same stem
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL', 'rougeLsum'], use_stemmer=True)

    rows = []
    
    # Accumulators for averaging
    scores_acc = {
        'rouge1_p': [], 'rouge1_r': [], 'rouge1_f': [],
        'rouge2_p': [], 'rouge2_r': [], 'rouge2_f': [],
        'rougeL_p': [], 'rougeL_r': [], 'rougeL_f': [],
        'rougeLsum_p': [], 'rougeLsum_r': [], 'rougeLsum_f': []
    }

    print(f"üöÄ Computing ROUGE metrics for {len(df)} samples...")

    for _, r in df.iterrows():
        question = str(r.get(q_col, '') or '')
        gen = str(r.get(a_col, '') or '')
        gt = str(r.get(gt_col, '') or '')
        qid = str(r.get(qid_col, '') or '') if qid_col else ''

        if not gen.strip() or not gt.strip():
            continue
        
        # Calculate scores
        # scorer.score(target, prediction)
        scores = scorer.score(gt, gen)
        
        row_data = {
            'Question_ID': qid,
            'Question': question,
            'Ground_Truth_Answer': gt,
            'Real_LLaMA_Answer': gen,
        }

        # Unpack scores
        for metric in ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']:
            s = scores[metric]
            row_data[f'{metric}_precision'] = s.precision
            row_data[f'{metric}_recall'] = s.recall
            row_data[f'{metric}_fmeasure'] = s.fmeasure
            
            scores_acc[f'{metric}_p'].append(s.precision)
            scores_acc[f'{metric}_r'].append(s.recall)
            scores_acc[f'{metric}_f'].append(s.fmeasure)

        rows.append(row_data)

    if not rows:
        print("‚ö†Ô∏è No evaluable rows (missing answers or ground truth).")
        return None

    # Calculate averages
    avg_scores = {k: np.mean(v) for k, v in scores_acc.items()}

    ts = datetime.now().strftime('%Y%m%d_%H%M%S')
    out_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'evaluation', 'ROUGE'))
    os.makedirs(out_dir, exist_ok=True)
    out_csv = os.path.join(out_dir, f"llama_evaluation_rouge_{ts}.csv")
    pd.DataFrame(rows).to_csv(out_csv, index=False)

    print("\n=== ROUGE Evaluation Summary ===")
    print(f"File: {results_csv}")
    print(f"Evaluated rows: {len(rows)}")
    print("-" * 40)
    print(f"ROUGE-1 (F1): {avg_scores['rouge1_f']:.4f}  (P: {avg_scores['rouge1_p']:.4f}, R: {avg_scores['rouge1_r']:.4f})")
    print(f"ROUGE-2 (F1): {avg_scores['rouge2_f']:.4f}  (P: {avg_scores['rouge2_p']:.4f}, R: {avg_scores['rouge2_r']:.4f})")
    print(f"ROUGE-L (F1): {avg_scores['rougeL_f']:.4f}  (P: {avg_scores['rougeL_p']:.4f}, R: {avg_scores['rougeL_r']:.4f})")
    print(f"ROUGE-Lsum (F1): {avg_scores['rougeLsum_f']:.4f}  (P: {avg_scores['rougeLsum_p']:.4f}, R: {avg_scores['rougeLsum_r']:.4f})")
    print("-" * 40)
    print(f"üìÅ Saved per-question ROUGE to: {out_csv}")
    return out_csv


if __name__ == '__main__':
    if len(sys.argv) < 2:
        print("Usage: python evaluate_answers_ROUGE.py <results_csv>")
        sys.exit(1)
    evaluate_rouge(sys.argv[1])
