#!/usr/bin/env python3
"""
BLEU Evaluator for Real Answers
Computes BLEU-1, BLEU-2, BLEU-3 for answers generated by generate_answers.py

Usage:
  python evaluate_answers_BLEU.py <results_csv>

This script expects a CSV with at least the following columns (robust to naming):
- Question (or 'question')
- Generated Answer (often: 'Answer' or 'Model_Answer' or 'Generated_Answer')
- Ground Truth (often: 'Ground Truth Answer', 'Ground_Truth', 'Ground_Truth_Answer', 'Answer - Reviewed')

Outputs a CSV in the evaluation folder with BLEU metrics per row, and prints macro averages.
"""

import os
import sys
import csv
import math
import time
from datetime import datetime
import pandas as pd
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from nltk.tokenize import word_tokenize
import nltk

# -------------------- Tokenization --------------------
def tokenize(text: str):
    t = (text or '').strip().lower()
    try:
        tokens = word_tokenize(t)
    except LookupError:
        try:
            nltk.download('punkt', quiet=True)
            nltk.download('punkt_tab', quiet=True)
            tokens = word_tokenize(t)
        except Exception:
            tokens = t.split()
    # filter out pure punctuation tokens (keep tokens containing alphanumerics)
    return [w for w in tokens if any(ch.isalnum() for ch in w)]

# -------------------- BLEU Implementation (NLTK) --------------------

def bleu_scores_nltk(candidate_tokens, reference_tokens):
    """Compute BLEU-1/2/3 using NLTK's sentence_bleu with smoothing.
    Weights:
      BLEU-1: (1, 0, 0, 0)
      BLEU-2: (0.5, 0.5, 0, 0)
      BLEU-3: (1/3, 1/3, 1/3, 0)
    """
    smoothie = SmoothingFunction().method1
    references = [reference_tokens]
    try:
        b1 = sentence_bleu(references, candidate_tokens, weights=(1, 0, 0, 0), smoothing_function=smoothie)
    except Exception:
        b1 = 0.0
    try:
        b2 = sentence_bleu(references, candidate_tokens, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothie)
    except Exception:
        b2 = 0.0
    try:
        b3 = sentence_bleu(references, candidate_tokens, weights=(1/3, 1/3, 1/3, 0), smoothing_function=smoothie)
    except Exception:
        b3 = 0.0
    return b1, b2, b3

# -------------------- Column resolution --------------------

def resolve_column(df, candidates, default=None):
    cols = {c.lower(): c for c in df.columns}
    for cand in candidates:
        key = cand.lower()
        if key in cols:
            return cols[key]
    return default

# -------------------- Main evaluation --------------------

def evaluate_bleu(results_csv: str):
    if not os.path.exists(results_csv):
        print(f"‚ùå CSV not found: {results_csv}")
        return None
    df = pd.read_csv(results_csv)
    # try to find columns
    q_col = resolve_column(df, ['Question', 'question'])
    a_col = resolve_column(df, ['Answer', 'Model_Answer', 'Generated_Answer', 'Answer_Text', 'Real_LLaMA_Answer'])
    gt_col = resolve_column(df, [
        'Ground Truth Answer', 'Ground_Truth', 'Ground_Truth_Answer', 'Answer - Reviewed', 'GroundTruth', 'Reviewed_Answer', 'Answer'
    ])
    qid_col = resolve_column(df, ['Question_ID', 'id', 'ID', 'QID', 'Question ID'])

    if not q_col or not a_col or not gt_col:
        print(f"‚ö†Ô∏è Columns not resolved. Found cols={list(df.columns)}")
        print(f"   Need at least Question, Generated Answer, and Ground Truth.")
        return None

    rows = []
    b1_sum = b2_sum = b3_sum = 0.0
    count = 0

    for _, r in df.iterrows():
        question = str(r.get(q_col, '') or '')
        gen = str(r.get(a_col, '') or '')
        gt = str(r.get(gt_col, '') or '')
        qid = str(r.get(qid_col, '') or '') if qid_col else ''

        if not gen.strip() or not gt.strip():
            continue
        cand_toks = tokenize(gen)
        ref_toks = tokenize(gt)
        b1, b2, b3 = bleu_scores_nltk(cand_toks, ref_toks)
        rows.append({
            'Question_ID': qid,
            'Question': question,
            'Ground_Truth_Answer': gt,
            'Real_LLaMA_Answer': gen,
            'BLEU_1': b1,
            'BLEU_2': b2,
            'BLEU_3': b3,
        })
        b1_sum += b1
        b2_sum += b2
        b3_sum += b3
        count += 1

    if count == 0:
        print("‚ö†Ô∏è No evaluable rows (missing answers or ground truth).")
        return None

    avg_b1 = b1_sum / count
    avg_b2 = b2_sum / count
    avg_b3 = b3_sum / count

    ts = datetime.now().strftime('%Y%m%d_%H%M%S')
    out_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'evaluation', 'BLEU'))
    os.makedirs(out_dir, exist_ok=True)
    out_csv = os.path.join(out_dir, f"llama_evaluation_bleu_{ts}.csv")
    pd.DataFrame(rows).to_csv(out_csv, index=False)

    print("=== BLEU Evaluation Summary ===")
    print(f"File: {results_csv}")
    print(f"Evaluated rows: {count}")
    print(f"BLEU-1 (avg): {avg_b1:.4f}")
    print(f"BLEU-2 (avg): {avg_b2:.4f}")
    print(f"BLEU-3 (avg): {avg_b3:.4f}")
    print(f"üìÅ Saved per-question BLEU to: {out_csv}")
    return out_csv


if __name__ == '__main__':
    if len(sys.argv) < 2:
        print("Usage: python evaluate_answers_BLEU.py <results_csv>")
        sys.exit(1)
    evaluate_bleu(sys.argv[1])
